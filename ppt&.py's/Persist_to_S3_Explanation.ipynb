{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Persisting DataFrame to Amazon S3 (Full Working Example)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "\n",
    "# ✅ 1. Initialize Spark with AWS and Hadoop configs\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PersistExample\")\n",
    "    # Include Hadoop AWS and SDK bundles\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.661\")\n",
    "    # Enable S3A FileSystem support\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    # Credential provider (auto-detects environment or ~/.aws/credentials)\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "    # Optional: specify region\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.ap-south-1.amazonaws.com\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"✅ SparkSession initialized successfully.\")\n",
    "\n",
    "# ✅ 2. Create sample DataFrame\n",
    "data = [\n",
    "    {\"name\": \"Aisha\", \"role\": \"Manager\", \"region\": \"APAC\"},\n",
    "    {\"name\": \"Rahul\", \"role\": \"Developer\", \"region\": \"EMEA\"},\n",
    "    {\"name\": \"Fatima\", \"role\": \"Analyst\", \"region\": \"APAC\"}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "# ✅ 3. Define your S3 output path\n",
    "# Make sure your AWS credentials have write access to this bucket\n",
    "output_path = \"s3a://training-bucket/etl/employees/\"\n",
    "\n",
    "print(\"\\nPersisting DataFrame to S3...\")\n",
    "\n",
    "# ✅ 4. Write DataFrame to S3 in Parquet format\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(\"✅ Data persisted to S3 successfully!\")\n",
    "\n",
    "# ✅ 5. Simulate and print what happens internally\n",
    "print(\"\\nWhat Happens Internally:\")\n",
    "print(\"\"\"\n",
    "1. Spark divides the DataFrame into partitions.\n",
    "2. Each executor writes its partition as a .parquet file.\n",
    "3. Files are uploaded to your S3 bucket using the Hadoop S3A connector.\n",
    "4. Spark writes a _SUCCESS marker after all files are uploaded.\n",
    "\"\"\")\n",
    "\n",
    "# ✅ 6. (Optional) Verify files in S3 using boto3\n",
    "try:\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = \"training-bucket\"\n",
    "    prefix = \"etl/employees/\"\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "    print(\"Files in S3:\")\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        print(\" -\", obj[\"Key\"])\n",
    "except Exception as e:\n",
    "    print(\"\\n⚠️ Could not verify S3 contents (simulation mode):\", e)\n",
    "\n",
    "# ✅ 7. Summary message\n",
    "print(\"\\nAll Done! Data has been persisted and verified successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
