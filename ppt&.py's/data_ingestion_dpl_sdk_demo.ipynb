{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9073db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example 1: Basic Data Ingestion Simulation\n",
    "------------------------------------------\n",
    "Simulates uploading a CSV file with token validation\n",
    "to a Data Lake folder using a mock DPL SDK behavior.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "# Setup\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger(\"dpl_ingestion_basic\")\n",
    "\n",
    "# Simulate DPL SDK upload\n",
    "def dpl_upload(csv_path, token, valid_token, target_path):\n",
    "    \"\"\"Mock function to simulate CSV upload with token verification.\"\"\"\n",
    "    if token != valid_token:\n",
    "        raise PermissionError(\"Invalid token. Upload denied.\")\n",
    "    shutil.copy(csv_path, target_path)\n",
    "    logger.info(f\"✅ File '{csv_path}' uploaded to {target_path}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"datalake/raw\", exist_ok=True)\n",
    "    open(\"staging_data.csv\", \"w\").write(\"id,name,region\\n1,Asha,APAC\\n2,Ravi,EMEA\")\n",
    "    token = \"ABC123TOKEN\"\n",
    "    valid_token = \"ABC123TOKEN\"\n",
    "\n",
    "    dpl_upload(\"staging_data.csv\", token, valid_token, \"datalake/raw/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3203e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example 2: Advanced DPL Publisher SDK Simulation with AWS S3\n",
    "------------------------------------------------------------\n",
    "Simulates real-world CSV ingestion into AWS S3 using a secure token.\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger(\"dpl_ingestion_advanced\")\n",
    "\n",
    "# Initialize S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "BUCKET_NAME = \"supplychain-data-lake\"\n",
    "VALID_TOKEN = \"TOKEN-XYZ-SECURE\"\n",
    "\n",
    "def upload_to_datalake(file_path, s3_key, token):\n",
    "    \"\"\"Upload CSV to S3 after verifying token.\"\"\"\n",
    "    if token != VALID_TOKEN:\n",
    "        logger.error(\"❌ Invalid token, upload rejected.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        s3.upload_file(file_path, BUCKET_NAME, s3_key)\n",
    "        logger.info(f\"✅ Successfully uploaded '{file_path}' to s3://{BUCKET_NAME}/{s3_key}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Upload failed: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create local file to upload\n",
    "    csv_content = \"id,city,shipment_count\\n1,Delhi,50\\n2,Paris,40\"\n",
    "    open(\"vendor_data.csv\", \"w\").write(csv_content)\n",
    "\n",
    "    # Upload using secure token\n",
    "    upload_to_datalake(\"vendor_data.csv\", \"raw/vendor_data.csv\", token=\"TOKEN-XYZ-SECURE\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
