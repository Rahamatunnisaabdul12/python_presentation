{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Topic: Data Pipeline Concepts (End-to-End Flow)\n",
    "===============================================\n",
    "Simulates a simple ETL pipeline using Python & boto3.\n",
    "Shows how AWS services like S3, Glue, and Athena can\n",
    "integrate to automate the full data flow.\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 1 ‚Äì Initialize Clients\n",
    "# -------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger(\"data_pipeline_demo\")\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "glue = boto3.client(\"glue\")\n",
    "athena = boto3.client(\"athena\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 2 ‚Äì Ingestion Simulation (Raw Data Upload)\n",
    "# -------------------------------------------------------------------\n",
    "def ingest_data(bucket, key, data):\n",
    "    \"\"\"Simulate raw data ingestion into S3.\"\"\"\n",
    "    logger.info(\"üì• Ingesting data into raw zone...\")\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=json.dumps(data))\n",
    "    logger.info(f\"‚úÖ Data uploaded to s3://{bucket}/{key}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 3 ‚Äì Trigger Glue Job (Transformation)\n",
    "# -------------------------------------------------------------------\n",
    "def run_glue_job(job_name):\n",
    "    \"\"\"Start AWS Glue job for transformation.\"\"\"\n",
    "    logger.info(f\"‚öôÔ∏è Starting Glue Job: {job_name}\")\n",
    "    response = glue.start_job_run(JobName=job_name)\n",
    "    job_run_id = response[\"JobRunId\"]\n",
    "    logger.info(f\"üÜî Glue JobRun ID: {job_run_id}\")\n",
    "    return job_run_id\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 4 ‚Äì Run Athena Query (Analytics Layer)\n",
    "# -------------------------------------------------------------------\n",
    "def run_athena_query(database, query, output_s3):\n",
    "    \"\"\"Execute Athena query and return results.\"\"\"\n",
    "    logger.info(\"üîç Running Athena query...\")\n",
    "    exec_id = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\"Database\": database},\n",
    "        ResultConfiguration={\"OutputLocation\": output_s3}\n",
    "    )[\"QueryExecutionId\"]\n",
    "\n",
    "    # Wait for completion\n",
    "    while True:\n",
    "        status = athena.get_query_execution(QueryExecutionId=exec_id)\n",
    "        state = status[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        if state in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n",
    "            break\n",
    "        time.sleep(2)\n",
    "\n",
    "    logger.info(f\"‚úÖ Query Status: {state}\")\n",
    "    return exec_id\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 5 ‚Äì Orchestration Simulation\n",
    "# -------------------------------------------------------------------\n",
    "def main():\n",
    "    bucket = \"supplychain-data-demo\"\n",
    "    raw_key = \"raw/shipments.json\"\n",
    "    glue_job_name = \"glue_transform_shipments\"\n",
    "    output_s3 = \"s3://aws-athena-query-results-demo/\"\n",
    "\n",
    "    # Step 1: Ingest Data\n",
    "    sample_data = {\n",
    "        \"shipment_id\": \"SHP001\",\n",
    "        \"status\": \"Delivered\",\n",
    "        \"region\": \"APAC\",\n",
    "        \"weight\": 12.4,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    ingest_data(bucket, raw_key, sample_data)\n",
    "\n",
    "    # Step 2: Transform Data (Glue Job)\n",
    "    run_glue_job(glue_job_name)\n",
    "\n",
    "    # Step 3: Query Data (Athena)\n",
    "    query = \"\"\"\n",
    "        SELECT region, COUNT(*) AS delivered_shipments\n",
    "        FROM trusted_shipments\n",
    "        WHERE status = 'Delivered'\n",
    "        GROUP BY region;\n",
    "    \"\"\"\n",
    "    run_athena_query(\"supplychain_catalog\", query, output_s3)\n",
    "\n",
    "    logger.info(\"üèÅ Data Pipeline executed successfully end-to-end.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
